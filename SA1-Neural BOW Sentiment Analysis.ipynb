{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(filename):\n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_docs(doc):\n",
    "   \n",
    "    # split data into tokens\n",
    "    words = doc.split()\n",
    "    \n",
    "    # load the punctuations to be removed\n",
    "    re_compile = re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    \n",
    "    # remove the punctuations from each word from the list of words\n",
    "    cleaned_text = [re_compile.sub('',w) for w in words]\n",
    "    \n",
    "    # remove all stop words from the text\n",
    "    \n",
    "    stop_words = stopwords.words('English')\n",
    "    cleaned_text = [word for word in cleaned_text if word not in stop_words]\n",
    "    \n",
    "    # remove all numbers or other characters which are not letters\n",
    "    \n",
    "    cleaned_text = [word for word in cleaned_text if word.isalpha()]\n",
    "    \n",
    "    # remove all characters or words with length <= 1\n",
    "    \n",
    "    cleaned_text = [word for word in cleaned_text if len(word) > 1]\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(filename, vocab):\n",
    "    \n",
    "    # after receving the filename , load the file from the list\n",
    "    text = load_docs(filename)\n",
    "    \n",
    "    # clean & return the loaded file\n",
    "    cleaned_text = clean_docs(text)\n",
    "    \n",
    "    # add all the cleaned words to the dictionary\n",
    "    vocabulary.update(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocabulary):\n",
    "    # iterate through all the files in the directory\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # add path to the file\n",
    "        path = directory+'/'+filename\n",
    "        \n",
    "        # pass the specific file to the below function to add the cleaned words in the file\n",
    "        # to the dictionary\n",
    "        \n",
    "        add_doc_to_vocab(path, vocabulary)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vocabulary(vocabulary, min_occurences):\n",
    "\n",
    "    tokens = [word for word,count in vocabulary.items() if count >= min_occurences]\n",
    "\n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(word_list, filename):\n",
    "    data = '\\n'.join(word_list)\n",
    "    file = open(filename,'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25767\n"
     ]
    }
   ],
   "source": [
    "vocabulary = Counter()\n",
    "negative_reviews = 'txt_sentoken/neg'\n",
    "positive_reviews = 'txt_sentoken/pos'\n",
    "min_occurences = 2\n",
    "reviews = [negative_reviews, positive_reviews]\n",
    "process_reviews = [ process_docs(review_directory,vocabulary)  for review_directory in reviews]\n",
    "\n",
    "tokens = process_vocabulary(vocabulary, min_occurences)\n",
    "save_list(tokens, \"vocabulary_10.txt\")\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Reviews to Lines of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_line(filename, vocabulary):\n",
    "    text = load_docs(filename)\n",
    "    \n",
    "    cleaned_text = clean_docs(text)\n",
    "    \n",
    "    tokens = [w for w in cleaned_text if w in vocabulary]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above lists the function doc to line() that will load a document, clean it, filter out tokens not in the vocabulary, then return the document as a string of white space separated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs_2(directory,vocabulary):\n",
    "    # iterate through all the files in the directory\n",
    "    lines = []\n",
    "    for filename in listdir(directory):\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # add path to the file\n",
    "        path = directory+'/'+filename\n",
    "        \n",
    "        # pass the specific file to the below function so that the cleaned words can be added as a list,\n",
    "        # so bascially each review will be a list appended to the master list\n",
    "        \n",
    "        line = doc_to_line(path, vocabulary)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above lists the process docs() function that does just this, expecting a directory name and a vocabulary set as input arguments and returning a list of processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_data(vocabulary):\n",
    "    negative_reviews_list = process_docs_2('txt_sentoken/neg',vocabulary)\n",
    "    positive_reviews_list = process_docs_2('txt_sentoken/pos',vocabulary)\n",
    "    \n",
    "    docs = negative_reviews_list + positive_reviews_list\n",
    "    labels = [0 for _ in range(len(negative_reviews_list))] + [1 for _ in range(len(positive_reviews_list))]\n",
    "    \n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = 'vocabulary_10.txt'\n",
    "vocabulary = load_docs(vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 : 1800\n"
     ]
    }
   ],
   "source": [
    "docs, labels = load_clean_data(vocabulary)\n",
    "print(len(docs), len(labels), sep= \" : \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Reviews to Bag Of Words Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens(docs):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs_3(directory, vocabulary, is_train):\n",
    "    # iterate through all the files in the directory\n",
    "    lines = []\n",
    "    for filename in listdir(directory):\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # add path to the file\n",
    "        path = directory+'/'+filename\n",
    "        \n",
    "        # pass the specific file to the below function so that the cleaned words can be added as a list,\n",
    "        # so bascially each review will be a list appended to the master list\n",
    "        \n",
    "        line = doc_to_line(path, vocabulary)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_data_2(vocabulary, is_train):\n",
    "    negative_reviews_list = process_docs_3('txt_sentoken/neg',vocabulary,is_train)\n",
    "    positive_reviews_list = process_docs_3('txt_sentoken/pos',vocabulary,is_train)\n",
    "    \n",
    "    docs = negative_reviews_list + positive_reviews_list\n",
    "    labels = np.array([0 for _ in range(len(negative_reviews_list))] + [1 for _ in range(len(positive_reviews_list))])\n",
    "    \n",
    "    return docs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, y_train = load_clean_data_2(vocabulary,True)\n",
    "test_docs, y_test = load_clean_data_2(vocabulary,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = create_tokens(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode = 'freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode = 'freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Here's what we have done so far:\n",
    "\n",
    " -  We had two folders : neg and pos\n",
    " -  In each of these folders there are 1000 files each, so all the files ranging from 0-899 in ether of the folders are taken as train dataset and the remaining 100, i.e. 900 to 999 are kept for test dataset\n",
    " - We go through each of the files kept for training and go through them to clean them and ultimately have a list of words, this list would be our entire vocabulary\n",
    " - Next we create a tokenizer, this tokenizer is fitted on the train dataset, which is a list of all the words in the train dataset, note that these words come from both the positive and negative reviews\n",
    " - Then we use the tokenizer.text_to_matrix to convert the train_doc to an array wherein each word is replaced by its frequency ( i.e. number of times a word appears in a document/ number of times it appears in all the docs)\n",
    " - We repeat the above step for the test doc too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(n_words):\n",
    "    model =Sequential()\n",
    "    model.add(Dense(50, input_shape = (n_words,), activation = 'relu'))\n",
    "    model.add(Dense(1,  activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/shekhartanwar/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/shekhartanwar/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.6919 - accuracy: 0.5839\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6830 - accuracy: 0.8917\n",
      "Epoch 3/10\n",
      " - 3s - loss: 0.6662 - accuracy: 0.8978\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.6401 - accuracy: 0.9222\n",
      "Epoch 5/10\n",
      " - 3s - loss: 0.6058 - accuracy: 0.9378\n",
      "Epoch 6/10\n",
      " - 3s - loss: 0.5662 - accuracy: 0.9383\n",
      "Epoch 7/10\n",
      " - 3s - loss: 0.5223 - accuracy: 0.9478\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.4792 - accuracy: 0.9494\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.4369 - accuracy: 0.9578\n",
      "Epoch 10/10\n",
      " - 3s - loss: 0.3962 - accuracy: 0.9639\n",
      "0.8700000047683716\n"
     ]
    }
   ],
   "source": [
    "n_words = Xtrain.shape[1]\n",
    "model = make_model(n_words)\n",
    "model.fit(Xtrain, y_train, batch_size = 32, epochs = 10, verbose = 2)\n",
    "loss, acc = model.evaluate(Xtest, y_test, verbose = 0)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Word Scoring Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(method, vocabulary):\n",
    "    train_docs, y_train = load_clean_data_2(vocabulary,True)\n",
    "    test_docs, y_test = load_clean_data_2(vocabulary,False)\n",
    "    tokenizer = create_tokens(train_docs)\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode = method)\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode = method)\n",
    "    return Xtrain,Xtest, y_train, y_test, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put the training and testing dataset preparation set in a function, and pass the method as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(n_words,Xtrain,Xtest, y_train, y_test):\n",
    "    n_repeats = 10\n",
    "    scores = []\n",
    "    for _ in range(n_repeats):\n",
    "        model = make_model(n_words)\n",
    "        model.fit(Xtrain, y_train, batch_size = 32, epochs = 10, verbose = 2)\n",
    "        _, acc = model.evaluate(Xtest, y_test, verbose = 0)\n",
    "        scores.append(acc)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 3s - loss: 0.4731 - accuracy: 0.7856\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0586 - accuracy: 0.9961\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.2482e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.9851e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4712 - accuracy: 0.7850\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.0555 - accuracy: 0.9967\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.8083e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.3560e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4920 - accuracy: 0.7606\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0742 - accuracy: 0.9900\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.7963e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.6836e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4572 - accuracy: 0.7889\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0534 - accuracy: 0.9961\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.6952e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.3087e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.7866e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4534 - accuracy: 0.8022\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0488 - accuracy: 0.9972\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.1774e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.1745e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4708 - accuracy: 0.7839\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0537 - accuracy: 0.9967\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.3874e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.2944e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4783 - accuracy: 0.7828\n",
      "Epoch 2/10\n",
      " - 3s - loss: 0.0571 - accuracy: 0.9950\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.7514e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.8502e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4881 - accuracy: 0.7644\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0672 - accuracy: 0.9933\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 7.3167e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 5.1135e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 3.7700e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4881 - accuracy: 0.7639\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0639 - accuracy: 0.9956\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 3s - loss: 9.5238e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.6154e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.8528e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4680 - accuracy: 0.7778\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0542 - accuracy: 0.9956\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 9.6865e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.9284e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.0813e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4659 - accuracy: 0.7772\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0555 - accuracy: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 9.9331e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4640 - accuracy: 0.7817\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0576 - accuracy: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 7.7048e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 5.8882e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4983 - accuracy: 0.7589\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0787 - accuracy: 0.9839\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.7669e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.3137e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.8029e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.4565 - accuracy: 0.7806\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0472 - accuracy: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 9.0240e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4607 - accuracy: 0.7800\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0561 - accuracy: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 8.0973e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 6.3203e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4531 - accuracy: 0.7883\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0476 - accuracy: 0.9944\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      " - 2s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 8.2372e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4601 - accuracy: 0.7833\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0653 - accuracy: 0.9878\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0164 - accuracy: 0.9994\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 9.4613e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4645 - accuracy: 0.7839\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0530 - accuracy: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.9808e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4784 - accuracy: 0.7650\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0610 - accuracy: 0.9911\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 9.9862e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 7.7728e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4578 - accuracy: 0.7867\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0534 - accuracy: 0.9939\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 3s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 8.4909e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 6.0843e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 4.5897e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4565 - accuracy: 0.7839\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.3596e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.1362e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.6760e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.6682e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.8943e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4565 - accuracy: 0.7806\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0149 - accuracy: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.1540e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.7574e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.9853e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.7789e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.0756e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4652 - accuracy: 0.7817\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0156 - accuracy: 0.9989\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.4115e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.1593e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.6638e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.6322e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.8503e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4799 - accuracy: 0.7628\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0176 - accuracy: 0.9989\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.9230e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.2747e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.6245e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.4851e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.7412e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4681 - accuracy: 0.7650\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0172 - accuracy: 0.9972\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.8076e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.6707e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.3040e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.3420e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.6584e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4539 - accuracy: 0.7839\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 9.0792e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.1401e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 4.2298e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.1628e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.4545e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.9479e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4644 - accuracy: 0.7683\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0126 - accuracy: 0.9989\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 8.0142e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.9155e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.5678e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.6062e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.8819e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4683 - accuracy: 0.7700\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0127 - accuracy: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 9.2285e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 6.5061e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 4.8533e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 3.6105e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.8571e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.3163e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4667 - accuracy: 0.7661\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0177 - accuracy: 0.9983\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0044 - accuracy: 0.9994\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 9.3072e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 6.4499e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.6735e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.5331e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 3s - loss: 2.7471e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.4576 - accuracy: 0.7878\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.0123 - accuracy: 0.9994\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 7.3619e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 2s - loss: 5.5261e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 2s - loss: 4.3050e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 2s - loss: 3.4301e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.7919e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6916 - accuracy: 0.5811\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6823 - accuracy: 0.5972\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6652 - accuracy: 0.7117\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6378 - accuracy: 0.8600\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6026 - accuracy: 0.8967\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5619 - accuracy: 0.9122\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5167 - accuracy: 0.9478\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4712 - accuracy: 0.9594\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4267 - accuracy: 0.9661\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3846 - accuracy: 0.9711\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6926 - accuracy: 0.5117\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6890 - accuracy: 0.6433\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6810 - accuracy: 0.8161\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6667 - accuracy: 0.9111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      " - 2s - loss: 0.6461 - accuracy: 0.9139\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.6185 - accuracy: 0.9161\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5857 - accuracy: 0.9417\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.5504 - accuracy: 0.9428\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.5131 - accuracy: 0.9539\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4767 - accuracy: 0.9561\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6915 - accuracy: 0.5550\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6823 - accuracy: 0.6733\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6656 - accuracy: 0.8006\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6414 - accuracy: 0.8606\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6101 - accuracy: 0.9211\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5741 - accuracy: 0.9428\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5355 - accuracy: 0.9506\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4955 - accuracy: 0.9533\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4552 - accuracy: 0.9600\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4168 - accuracy: 0.9667\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6919 - accuracy: 0.5367\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6829 - accuracy: 0.5611\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6642 - accuracy: 0.7778\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6347 - accuracy: 0.8828\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5973 - accuracy: 0.9217\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5541 - accuracy: 0.9361\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5087 - accuracy: 0.9533\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4626 - accuracy: 0.9578\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4178 - accuracy: 0.9617\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3756 - accuracy: 0.9722\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6915 - accuracy: 0.5467\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6810 - accuracy: 0.6172\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6618 - accuracy: 0.7333\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6310 - accuracy: 0.9050\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5918 - accuracy: 0.9189\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5478 - accuracy: 0.9272\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5017 - accuracy: 0.9489\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4562 - accuracy: 0.9589\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4115 - accuracy: 0.9633\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3703 - accuracy: 0.9683\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6921 - accuracy: 0.5978\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6852 - accuracy: 0.7533\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6704 - accuracy: 0.8700\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6457 - accuracy: 0.9067\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6135 - accuracy: 0.9272\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5749 - accuracy: 0.9378\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5331 - accuracy: 0.9428\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4904 - accuracy: 0.9489\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4483 - accuracy: 0.9556\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4078 - accuracy: 0.9644\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6919 - accuracy: 0.5178\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6832 - accuracy: 0.5983\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6664 - accuracy: 0.6489\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6418 - accuracy: 0.7778\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6094 - accuracy: 0.8428\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5719 - accuracy: 0.8944\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5316 - accuracy: 0.9333\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4882 - accuracy: 0.9511\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4464 - accuracy: 0.9606\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4059 - accuracy: 0.9689\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6920 - accuracy: 0.5256\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6849 - accuracy: 0.6917\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6697 - accuracy: 0.8706\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6448 - accuracy: 0.9111\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6117 - accuracy: 0.9239\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5712 - accuracy: 0.9400\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5290 - accuracy: 0.9400\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4840 - accuracy: 0.9528\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4414 - accuracy: 0.9539\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4008 - accuracy: 0.9622\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6919 - accuracy: 0.5561\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6844 - accuracy: 0.7039\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6687 - accuracy: 0.8322\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6436 - accuracy: 0.9272\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.6090 - accuracy: 0.9344\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5694 - accuracy: 0.9383\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5260 - accuracy: 0.9394\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4834 - accuracy: 0.9544\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4396 - accuracy: 0.9556\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3989 - accuracy: 0.9628\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6912 - accuracy: 0.5461\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6799 - accuracy: 0.8344\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.6596 - accuracy: 0.7217\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.6303 - accuracy: 0.9272\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.5928 - accuracy: 0.9294\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.5505 - accuracy: 0.9428\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.5061 - accuracy: 0.9456\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4609 - accuracy: 0.9561\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4188 - accuracy: 0.9556\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.3785 - accuracy: 0.9650\n"
     ]
    }
   ],
   "source": [
    "methods = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = pd.DataFrame()\n",
    "for method in methods:\n",
    "    Xtrain,Xtest, y_train, y_test, tokenizer = prepare_train_test_data(method, vocabulary)\n",
    "    n_words = Xtrain.shape[1]\n",
    "    results[method] = evaluate_model(n_words,Xtrain,Xtest, y_train, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binary</th>\n",
       "      <th>count</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.930</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.935</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.925</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.930</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.935</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.930</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.915</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   binary  count  tfidf   freq\n",
       "0   0.930  0.910  0.905  0.870\n",
       "1   0.935  0.910  0.885  0.860\n",
       "2   0.925  0.915  0.885  0.875\n",
       "3   0.930  0.890  0.860  0.865\n",
       "4   0.935  0.900  0.885  0.870\n",
       "5   0.920  0.905  0.870  0.865\n",
       "6   0.920  0.905  0.850  0.855\n",
       "7   0.930  0.905  0.910  0.870\n",
       "8   0.920  0.900  0.885  0.865\n",
       "9   0.915  0.900  0.860  0.870"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binary</th>\n",
       "      <th>count</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.879500</td>\n",
       "      <td>0.866500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006992</td>\n",
       "      <td>0.006992</td>\n",
       "      <td>0.019501</td>\n",
       "      <td>0.005798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.867500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.908750</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.935000</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          binary      count      tfidf       freq\n",
       "count  10.000000  10.000000  10.000000  10.000000\n",
       "mean    0.926000   0.904000   0.879500   0.866500\n",
       "std     0.006992   0.006992   0.019501   0.005798\n",
       "min     0.915000   0.890000   0.850000   0.855000\n",
       "25%     0.920000   0.900000   0.862500   0.865000\n",
       "50%     0.927500   0.905000   0.885000   0.867500\n",
       "75%     0.930000   0.908750   0.885000   0.870000\n",
       "max     0.935000   0.915000   0.910000   0.875000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentiment(text, vocabulary, model, tokenizer):\n",
    "       \n",
    "#     cleaned_text = clean_docs(text)\n",
    "\n",
    "#     cleaned_text = [w for w in cleaned_text if w in vocabulary]\n",
    "\n",
    "#     line = ' '.join(cleaned_text)\n",
    "\n",
    "#     encoded = tokenizer.texts_to_matrix([line], mode = 'binary')\n",
    "    \n",
    "#     #print(model.summmary())\n",
    "# #     y_hat = model.predict(encoded, verbose = 0)\n",
    "\n",
    "# #     percent_positive = y_hat[0,0]\n",
    "    \n",
    "# #     if round(percent_positive) == 0:\n",
    "# #         return (1-percent_positive),\"Negative\"\n",
    "# #     return percent_positive,\"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.3588 - accuracy: 0.9639\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.3259 - accuracy: 0.9733\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.2940 - accuracy: 0.9800\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.2665 - accuracy: 0.9822\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.2412 - accuracy: 0.9861\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.2193 - accuracy: 0.9861\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.1985 - accuracy: 0.9906\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.1805 - accuracy: 0.9917\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.1648 - accuracy: 0.9928\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.1496 - accuracy: 0.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a5ad84150>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, y_train, batch_size = 32, epochs = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'Best movie ever! It was great, I recommend it.'\n",
    "# predict_sentiment(text, vocabulary, tokenizer, model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'Best movie ever! It was great, I recommend it.'\n",
    "# percent, sentiment = predict_sentiment(text, vocabulary, tokenizer, model) \n",
    "# print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100)) \n",
    "# # test negative text\n",
    "# text = 'This is a bad movie.'\n",
    "# percent, sentiment = predict_sentiment(text, vocabulary, tokenizer, model) \n",
    "# print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 : Positive\n"
     ]
    }
   ],
   "source": [
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "cleaned_text = clean_docs(text)\n",
    "\n",
    "cleaned_text = [w for w in cleaned_text if w in vocabulary]\n",
    "\n",
    "line = ' '.join(cleaned_text)\n",
    "\n",
    "encoded = tokenizer.texts_to_matrix([line], mode = 'binary')\n",
    "\n",
    "y_hat = model.predict(encoded, verbose = 0)\n",
    "\n",
    "percent_positive = y_hat[0,0]\n",
    "\n",
    "if round(percent_positive) == 0:\n",
    "    print ((1-percent_positive),\"Negative\", sep= \" : \")\n",
    "else:\n",
    "    print(percent_positive,\"Positive\", sep= \" : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999945189377 : Negative\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a bad movie.'\n",
    "cleaned_text = clean_docs(text)\n",
    "\n",
    "cleaned_text = [w for w in cleaned_text if w in vocabulary]\n",
    "\n",
    "line = ' '.join(cleaned_text)\n",
    "\n",
    "encoded = tokenizer.texts_to_matrix([line], mode = 'binary')\n",
    "\n",
    "y_hat = model.predict(encoded, verbose = 0)\n",
    "\n",
    "percent_positive = y_hat[0,0]\n",
    "\n",
    "if round(percent_positive) == 0:\n",
    "    print ((1-percent_positive),\"Negative\", sep= \" : \")\n",
    "else:\n",
    "    print(percent_positive,\"Positive\", sep= \" : \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
