{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Clean the text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename,'r')\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(data):\n",
    "    tokens = data.split()\n",
    "    \n",
    "    re_punct = re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    \n",
    "    cleaned_text = [re_punct.sub('',w) for w in tokens]\n",
    "    \n",
    "    cleaned_text = [word for word in cleaned_text if word.isalpha()]\n",
    "    \n",
    "    stop_words =  stopwords.words('english')\n",
    "    cleaned_text = [word for word in cleaned_text if word not in stop_words]\n",
    "    \n",
    "    cleaned_text = [word for word in cleaned_text if len(word) > 1]\n",
    "    \n",
    "    return cleaned_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Vocbulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_doc_to_vocab(filepath, vocabulary):\n",
    "    data = load_doc(filepath)\n",
    "    clean_text = clean_doc(data)\n",
    "    vocabulary.update(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs(directory,vocabulary):\n",
    "   \n",
    "    for file in listdir(directory):\n",
    "        if file.startswith('cv9'):\n",
    "            continue     \n",
    "      \n",
    "        filepath = directory+'/'+file\n",
    "      \n",
    "        add_doc_to_vocab(filepath, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vocabulary(min_occurences,vocabulary):\n",
    "    tokens = [word for word,count in vocabulary.items() if count >= min_occurences]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(word_list, file_name):\n",
    "    string = '\\n'.join(word_list)\n",
    "    file = open(file_name,'w')\n",
    "    file.write(string)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Counter()\n",
    "negative_reviews = 'txt_sentoken/neg'\n",
    "positive_reviews = 'txt_sentoken/pos'\n",
    "min_occurences = 2\n",
    "reviews = [negative_reviews, positive_reviews]\n",
    "for directory in reviews:\n",
    "    process_docs(directory,vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurences = 2\n",
    "tokens = process_vocabulary(min_occurences,vocabulary)\n",
    "save_list(tokens, 'vocabulary_new.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN with Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc_vocab(data, vocabulary):\n",
    "    tokens = data.split()\n",
    "    \n",
    "    re_punct = re.compile('[%s]'%re.escape(string.punctuation))\n",
    "    \n",
    "    cleaned_text = [re_punct.sub('',w) for w in tokens]\n",
    "    \n",
    "    cleaned_text = [word for word in cleaned_text if word in vocabulary]\n",
    "    \n",
    "    tokens = ' '.join(cleaned_text)\n",
    "    \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs_2(directory, vocabulary, is_train):\n",
    "    documents = []\n",
    "    for file in listdir(directory):\n",
    "        if is_train and file.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not file.startswith('cv9'):\n",
    "            continue\n",
    "        \n",
    "        file_path = directory+'/'+file\n",
    "        \n",
    "        data = load_doc(file_path)\n",
    "        \n",
    "        cleaned_data = clean_doc_vocab(data,vocabulary)\n",
    "        \n",
    "        documents.append(cleaned_data)\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_docs(vocabulary, is_train):\n",
    "    negative_reviews = 'txt_sentoken/neg'\n",
    "    positive_reviews = 'txt_sentoken/pos'\n",
    "    \n",
    "    neg_docs = process_docs_2(negative_reviews, vocabulary, is_train)\n",
    "    pos_docs = process_docs_2(positive_reviews, vocabulary, is_train)\n",
    "    \n",
    "    docs = neg_docs + pos_docs\n",
    "    \n",
    "    labels = np.array([0 for _ in range(len(neg_docs))] +  [1 for _ in range(len(pos_docs))])\n",
    "    \n",
    "    return docs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pad(tokenizer, docs, max_len):\n",
    "   \n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    padded_sequence = pad_sequences(encoded, maxlen = max_len, padding = 'post')\n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(vocabulary_size, output_dimension_size, input_sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocabulary_size, output_dim = output_dimension_size, input_length= input_sequence_length))\n",
    "    model.add(Conv1D(filters = 32, kernel_size = 8, activation = 'relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss= 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = True\n",
    "vocabulary = load_doc('vocabulary_new.txt')\n",
    "voacbulary = set(vocabulary.split())\n",
    "train_docs,train_labels = load_clean_docs(vocabulary, is_train)\n",
    "max_len = max([len(item.split()) for item in train_docs])\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "X_train = encode_pad(tokenizer, train_docs,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27675"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2282"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 29s - loss: 0.6921 - accuracy: 0.5389\n",
      "Epoch 2/10\n",
      " - 31s - loss: 0.5305 - accuracy: 0.7561\n",
      "Epoch 3/10\n",
      " - 29s - loss: 0.1179 - accuracy: 0.9683\n",
      "Epoch 4/10\n",
      " - 29s - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 30s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 29s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 28s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 30s - loss: 8.1593e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 29s - loss: 6.3230e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 29s - loss: 5.0176e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a726f39d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_index)+1\n",
    "output_dimension_size = 100\n",
    "input_sequence_length = max_len\n",
    "model = make_model(vocabulary_size,output_dimension_size, input_sequence_length)\n",
    "model.fit(X_train, train_labels, epochs = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acccuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "_,acc = model.evaluate(X_train,train_labels, verbose = 2)\n",
    "print(\"Train Acccuracy : \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train =  False\n",
    "test_docs,test_labels = load_clean_docs(vocabulary, is_train)\n",
    "X_test= encode_pad(tokenizer, test_docs, max_len)\n",
    "_,acc = model.evaluate(X_test,test_labels, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('train_data.out',X_train)\n",
    "np.savetxt('train_labels.out',train_labels)\n",
    "np.savetxt('test_data.out',X_test)\n",
    "np.savetxt('test_labels.out',test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acccuracy :  0.8899999856948853\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Acccuracy : \",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, max_length, model): # clean review\n",
    "    line = clean_doc_vocab(review, vocab)\n",
    "   \n",
    "    # encode and pad review\n",
    "    padded = encode_pad(tokenizer, [line],  max_length)\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    percent_neg = 1- percent_pos\n",
    "    \n",
    "    if percent_pos > percent_neg:\n",
    "        return percent_pos,\"Positive\"\n",
    "    else:\n",
    "        return percent_neg, \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Everyone will enjoy this film. I relaly like it and my family did too. I love it, recommended!]\n",
      "Sentiment: Positive (61.623%)\n",
      "Review: [This is a bad movie. Do not watch fucking watch it. Thge acting and storyline is terrible.It sucks.]\n",
      "Sentiment: Negative (64.164%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Everyone will enjoy this film. I relaly like it and my family did too. I love it, recommended!'\n",
    "model = load_model('model_cnn.h5')\n",
    "percent, sentiment = predict_sentiment(text, vocabulary, tokenizer, max_len, model) \n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch fucking watch it. Thge acting and storyline is terrible.It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocabulary, tokenizer, max_len, model) \n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
